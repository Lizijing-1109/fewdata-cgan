
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import pandas as pd
import numpy as np
import matplotlib

matplotlib.use('Agg')
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.preprocessing import StandardScaler
from scipy import stats
from scipy.interpolate import griddata
import os
import seaborn as sns
import statsmodels.api as sm

# ä¸­æ–‡æ˜¾ç¤º
import matplotlib as mpl

font_path = r'C:\Windows\Fonts\simhei.ttf'
if os.path.exists(font_path):
    mpl.rcParams['font.family'] = 'sans-serif'
    mpl.rcParams['font.sans-serif'] = ['SimHei']
mpl.rcParams['axes.unicode_minus'] = False


# --------------------  ç½‘ç»œç»“æ„ (ä¿æŒä¸å˜: æ¡ä»¶ 2D, è¾“å‡º 3D)  --------------------
class Generator(nn.Module):
    def __init__(self, input_size=4, condition_size=2, hidden_size=128, output_size=3):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(input_size + condition_size, hidden_size),
            nn.BatchNorm1d(hidden_size), nn.ReLU(),
            nn.Linear(hidden_size, hidden_size),
            nn.BatchNorm1d(hidden_size), nn.ReLU(),
            nn.Linear(hidden_size, hidden_size),
            nn.BatchNorm1d(hidden_size), nn.ReLU(),
            nn.Linear(hidden_size, hidden_size),
            nn.BatchNorm1d(hidden_size), nn.ReLU(),
            nn.Linear(hidden_size, output_size)
        )

    def forward(self, x, c):
        return self.model(torch.cat([x, c], 1))


class Discriminator(nn.Module):
    def __init__(self, input_size=3, condition_size=2, hidden_size=128):
        super().__init__()
        COND_FEAT_SIZE = 64  # ç¡®ä¿ä¸æ¨¡å‹å®šä¹‰å…¼å®¹
        INPUT_COMBINED_SIZE = input_size + COND_FEAT_SIZE  # 3 + 64 = 67

        self.cond_layer = nn.Sequential(
            nn.Linear(condition_size, COND_FEAT_SIZE),
            nn.LeakyReLU(0.2)
        )

        self.model = nn.Sequential(
            nn.Linear(INPUT_COMBINED_SIZE, hidden_size * 4), nn.LeakyReLU(0.2),
            nn.Linear(hidden_size * 4, hidden_size * 2), nn.LayerNorm(hidden_size * 2), nn.LeakyReLU(0.2),
            nn.Linear(hidden_size * 2, hidden_size), nn.LeakyReLU(0.2),
            nn.Linear(hidden_size, 1)
        )
        self.cond_pred = nn.Sequential(
            nn.Linear(INPUT_COMBINED_SIZE, hidden_size), nn.LeakyReLU(0.2),
            nn.Linear(hidden_size, condition_size)
        )

    def forward(self, x, c):
        c_feat = self.cond_layer(c)
        comb = torch.cat([x, c_feat], 1)
        validity = self.model(comb)
        pred_c = self.cond_pred(comb)
        return validity, pred_c


# --------------------  æ•°æ® (é‡ç‚¹ä¿®æ”¹æ­¤éƒ¨åˆ†) --------------------
class CustomDataset(Dataset):
    def __init__(self, excel_file_path, scaler=None):
        df = pd.read_excel(excel_file_path)

        # ğŸŸ¢ ä¿®æ­£æ¡ä»¶: è¯»å– Excel ç¬¬ 1, 2 åˆ— (ç´¢å¼• 0:2) -> 2 ç»´
        self.conditions = df.iloc[:, 0:2].values.astype(np.float32)

        # ğŸŸ¢ ä¿®æ­£æ ‡ç­¾: è¯»å– Excel ç¬¬ 3, 4, 5 åˆ— (ç´¢å¼• 2:5) -> 3 ç»´
        self.labels = df.iloc[:, 2:5].values.astype(np.float32)

        if scaler is None:
            self.scaler = {
                'conditions': StandardScaler().fit(self.conditions),
                'labels': StandardScaler().fit(self.labels)
            }
        else:
            self.scaler = scaler
        self.conditions = self.scaler['conditions'].transform(self.conditions)
        self.labels = self.scaler['labels'].transform(self.labels)

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return torch.tensor(self.conditions[idx]), torch.tensor(self.labels[idx])


# --------------------  æ¢¯åº¦æƒ©ç½š (ä¸å˜) --------------------
def compute_gradient_penalty(D, real, fake, cond):
    alpha = torch.rand(real.size(0), 1).expand_as(real).to(real.device)
    interp = (alpha * real + (1 - alpha) * fake).requires_grad_(True)
    d_interp, _ = D(interp, cond)
    grad = torch.autograd.grad(
        outputs=d_interp, inputs=interp,
        grad_outputs=torch.ones_like(d_interp),
        create_graph=True, retain_graph=True, only_inputs=True
    )[0]
    return ((grad.view(grad.size(0), -1).norm(2, dim=1) - 1) ** 2).mean()


# --------------------  éªŒè¯å‡½æ•° (ä¸å˜) --------------------
def validate_generator(generator, loader, device, scaler):
    generator.eval()
    all_real, all_pred, all_cond = [], [], []
    with torch.no_grad():
        for cond, real in loader:
            cond, real = cond.to(device), real.to(device)
            noise = torch.randn(cond.size(0), 4).to(device)
            pred = generator(noise, cond)
            all_real.append(scaler['labels'].inverse_transform(real.cpu().numpy()))
            all_pred.append(scaler['labels'].inverse_transform(pred.cpu().numpy()))
            all_cond.append(scaler['conditions'].inverse_transform(cond.cpu().numpy()))

    all_real = np.concatenate(all_real, 0)
    all_pred = np.concatenate(all_pred, 0)
    all_cond = np.concatenate(all_cond, 0)

    # é€æŒ‡æ ‡è®¡ç®—
    r2_each = r2_score(all_real, all_pred, multioutput='raw_values')
    mae_each = mean_absolute_error(all_real, all_pred, multioutput='raw_values')
    mse_each = ((all_real - all_pred) ** 2).mean(axis=0)
    mape_each = (np.abs((all_real - all_pred) / (all_real + 1e-8))).mean(axis=0) * 100

    # æ•´ä½“å¹³å‡
    average_r2 = r2_score(all_real, all_pred, multioutput='uniform_average')
    average_mae = mae_each.mean()
    average_mse = mse_each.mean()

    # æ‰“å°æ±‡æ€»
    print(f"Validation MSE: {average_mse:.4f}")
    print(f"Validation MAE: {average_mae:.4f}")
    print(f"Validation RÂ² (Overall): {average_r2:.4f}")
    print("\nå„ç‰¹å¾ RÂ²:", r2_each)
    print("å„ç‰¹å¾ MSE:", mse_each)
    print("å„ç‰¹å¾ MAE:", mae_each)
    print("\nç›¸å¯¹è¯¯å·®ï¼ˆMAPE %ï¼‰:")
    for i, err in enumerate(mape_each):
        print(f"ç‰¹å¾ {i + 1}: {err:.2f}%")

    # ======  ä¸‰ç»´ç½®ä¿¡åŒºé—´å›¾ï¼ˆ3 ä¸ªè¾“å‡ºï¼‰  ======
    feature_names_zh = ['å¯¼çƒ­ç‡', 'å¼¹æ€§æ¨¡é‡', 'çƒ­è†¨èƒ€ç‡']
    for i in range(3):
        fig = plt.figure(figsize=(10, 8))
        ax = fig.add_subplot(111, projection='3d')

        unique_cond = np.unique(all_cond, axis=0)
        for j, c in enumerate(unique_cond):
            idx = np.all(all_cond == c, axis=1)
            pred_vals = all_pred[idx, i]
            mean_val = pred_vals.mean()
            sem = pred_vals.std(ddof=1) / np.sqrt(pred_vals.size)
            half_width = stats.t.ppf(0.975, pred_vals.size - 1) * sem
            # è¯¯å·®æ¡
            ax.plot([c[0], c[0]], [c[1], c[1]],
                    [mean_val - half_width, mean_val + half_width],
                    color='blue', alpha=0.6)
            ax.scatter(c[0], c[1], mean_val, color='blue', s=25,
                       label='é¢„æµ‹å‡å€¼' if j == 0 else "")
            # çœŸå®å€¼
            ax.scatter(c[0], c[1], all_real[idx, i].mean(),
                       color='red', s=25, label='çœŸå®å€¼' if j == 0 else "")

        ax.set_xlabel('Condition 1 (Excel Col 1)')
        ax.set_ylabel('Condition 2 (Excel Col 2)')
        ax.set_zlabel(f'{feature_names_zh[i]} å€¼')
        ax.set_title(f'{feature_names_zh[i]} éšå·¥è‰ºæ¡ä»¶å˜åŒ–çš„ä¸‰ç»´ç½®ä¿¡åŒºé—´')
        ax.legend(loc='upper right', bbox_to_anchor=(1.25, 1))
        plt.tight_layout()
        plt.savefig(f'3D_{feature_names_zh[i]}_ç½®ä¿¡åŒºé—´_vs_æ¡ä»¶.png', dpi=300)
        plt.close()
    # ======  ä¸­æ–‡ä¿®æ”¹ç»“æŸ  ======

    # åç»­å›¾ä¿æŒåŸæ ·ï¼ˆ3 ä¸ªå­å›¾ï¼‰
    feature_names = ['çƒ­å¯¼ç‡', 'å¼¹æ€§æ¨¡é‡', 'çƒ­è†¨èƒ€ç‡']
    plt.figure(figsize=(18, 5))
    for i in range(3):
        plt.subplot(1, 3, i + 1)
        plt.scatter(all_real[:, i], all_pred[:, i], alpha=0.6, s=15)
        mi, ma = all_real[:, i].min(), all_real[:, i].max()
        plt.plot([mi, ma], [mi, ma], 'r--')
        plt.xlabel('Real');
        plt.ylabel('Predicted')
        plt.title(f'{feature_names[i]}: Real vs Predicted')
        plt.grid(True)
    plt.tight_layout();
    plt.savefig('åˆå¹¶_çƒ­å¯¼ç‡_å¼¹æ€§æ¨¡é‡_çƒ­è†¨èƒ€ç‡_Real_vs_Predicted.png', dpi=300);
    plt.close()

    # çƒ­åŠ›å›¾
    corr = np.corrcoef(np.hstack((all_cond, all_real)).T)
    names = [f'Cond{i + 1}' for i in range(all_cond.shape[1])] + [f'Target{i + 1}' for i in range(3)]
    plt.figure(figsize=(12, 10))
    sns.heatmap(corr, annot=True, cmap='coolwarm', xticklabels=names, yticklabels=names)
    plt.title('Correlation Matrix Heatmap')
    plt.tight_layout();
    plt.savefig('Correlation_Heatmap.png');
    plt.close()

    return r2_score(all_real, all_pred, multioutput='raw_values')


# --------------------  ä¸»è®­ç»ƒ (ä¿æŒä¸å˜) --------------------
batch_size = 128
num_epochs = 200
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# æ•°æ®
train_set = CustomDataset(r'C:/Users/admin/Desktop/LEARNING-data.xlsx')
train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)

# æ¨¡å‹
G = Generator(4, 2, 256, 3).to(device)
D = Discriminator(3, 2, 256).to(device)
g_opt = optim.RMSprop(G.parameters(), lr=1e-4)
d_opt = optim.RMSprop(D.parameters(), lr=1e-4)
g_sched = optim.lr_scheduler.ExponentialLR(g_opt, 0.98)
d_sched = optim.lr_scheduler.ExponentialLR(d_opt, 0.98)

# è®­ç»ƒå¾ªç¯
for epoch in range(num_epochs):
    for cond, real in train_loader:
        cond, real = cond.to(device), real.to(device)
        batch = cond.size(0)
        noise = torch.randn(batch, 4).to(device)

        # åˆ¤åˆ«å™¨
        D.zero_grad()
        real_valid, _ = D(real, cond)
        fake = G(noise, cond)
        fake_valid, _ = D(fake.detach(), cond)
        d_loss = -real_valid.mean() + fake_valid.mean() + \
                 5 * compute_gradient_penalty(D, real, fake, cond)
        d_loss.backward()
        d_opt.step()

        # ç”Ÿæˆå™¨
        G.zero_grad()
        fake = G(noise, cond)
        fake_valid, _ = D(fake, cond)
        g_loss = -fake_valid.mean() + 10 * nn.L1Loss()(fake, real)
        g_loss.backward()
        g_opt.step()

    g_sched.step()
    d_sched.step()
    if epoch % 10 == 0:
        print(f'Epoch {epoch:03d}  D-loss {d_loss.item():.4f}  G-loss {g_loss.item():.4f}')

# ä¿å­˜æ¨¡å‹
torch.save(G.state_dict(), 'generator_model.pth')

# éªŒè¯
val_set = CustomDataset(r'C:/Users/admin/Desktop/valid-FILE', scaler=train_set.scaler)
val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)
validate_generator(G, val_loader, device, train_set.scaler)
